{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"c6cad3e4","cell_type":"code","source":"import gc\nimport os\nimport random\n\nimport numpy as np\nimport torch\nimport triton\nimport triton.language as tl\n\nfrom datasets import load_dataset\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE': 128}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 256}, num_stages=1, num_warps=8),\n        triton.Config({'BLOCK_SIZE': 512}, num_stages=1, num_warps=8),\n    ],\n    key=['n_cols'],\n)\n@triton.jit\ndef _quantize_rowwise_int4(\n    x_ptr, output_ptr, output_maxs, n_rows, n_cols,\n    BLOCK_SIZE: tl.constexpr\n):\n\n    row_idx = tl.program_id(0)\n    if row_idx >= n_rows:\n        return\n    \n    row_start = row_idx * n_cols\n    \n    col_offsets = tl.arange(0, BLOCK_SIZE)\n    mask = col_offsets < n_cols\n    row = tl.load(x_ptr + row_start + col_offsets, mask=mask, other=0.0)\n    \n    abs_row = tl.abs(row)\n    row_max = tl.max(tl.where(mask, abs_row, 0.0))\n    \n    row_max_safe = tl.maximum(row_max, 1e-12)\n    scale = 8.0 / row_max_safe\n    \n    \n    tl.store(output_maxs + row_idx, row_max_safe.to(tl.float16))\n    \n    packed_row_stride = n_cols // 2\n    packed_row_start = row_idx * packed_row_stride\n    \n    for packed_start in range(0, packed_row_stride, BLOCK_SIZE):\n        packed_offs = packed_start + tl.arange(0, BLOCK_SIZE)\n        mask_packed = packed_offs < packed_row_stride\n        \n        orig_offs = packed_offs * 2\n        mask1 = orig_offs < n_cols\n        mask2 = (orig_offs + 1) < n_cols\n        \n        val1 = tl.load(x_ptr + row_start + orig_offs, mask=mask1 & mask_packed, other=0.0)\n        val2 = tl.load(x_ptr + row_start + orig_offs + 1, mask=mask2 & mask_packed, other=0.0)\n        \n        quant1 = tl.extra.cuda.libdevice.rint(val1 * scale)\n        quant2 = tl.extra.cuda.libdevice.rint(val2 * scale)\n        \n        quant1 = tl.minimum(tl.maximum(quant1, -8.0), 7.0)\n        quant2 = tl.minimum(tl.maximum(quant2, -8.0), 7.0)\n        \n        uint4_1 = (quant1 + 8).to(tl.uint8)\n        uint4_2 = (quant2 + 8).to(tl.uint8)\n        packed = (uint4_1 << 4) | uint4_2\n        \n        tl.store(output_ptr + packed_row_start + packed_offs, packed, mask=mask_packed)\n\ndef quantize_rowwise_int4(x: torch.Tensor):\n    assert x.is_cuda and x.dim() == 2\n    n_rows, n_cols = x.shape\n    assert n_cols % 2 == 0\n    \n    packed_n_cols = n_cols // 2\n    q_packed = torch.empty((n_rows, packed_n_cols), device=x.device, dtype=torch.uint8)\n    absmaxs = torch.empty(n_rows, device=x.device, dtype=torch.float16)\n    \n    grid = (n_rows,)\n    _quantize_rowwise_int4[grid](x, q_packed, absmaxs, n_rows, n_cols)\n    \n    return q_packed, absmaxs\n\ndef dequantize_rowwise_int4(packed: torch.Tensor, absmaxs: torch.Tensor):\n    assert packed.is_cuda and absmaxs.is_cuda\n    assert packed.dtype == torch.uint8\n    assert absmaxs.dtype == torch.float16\n    assert packed.dim() == 2 and absmaxs.dim() == 1\n    assert packed.size(0) == absmaxs.size(0)\n    \n    n_rows, packed_n_cols = packed.shape\n    n_cols = packed_n_cols * 2\n    \n    uint4_1 = (packed >> 4) & 0x0F\n    uint4_2 = packed & 0x0F\n    \n    int4_1 = uint4_1.to(torch.int8) - 8\n    int4_2 = uint4_2.to(torch.int8) - 8\n    \n    dequantized = torch.empty((n_rows, n_cols), device=packed.device, dtype=torch.float16)\n    dequantized[:, 0::2] = int4_1.to(torch.float16)\n    dequantized[:, 1::2] = int4_2.to(torch.float16)\n    \n    scale = (absmaxs / 8.0).unsqueeze(1)\n    dequantized = dequantized * scale\n    \n    return dequantized\n\ndef matmul_fp16_int4(a: torch.Tensor, b_packed: torch.Tensor, absmaxs: torch.Tensor):\n    assert a.is_cuda and b_packed.is_cuda and absmaxs.is_cuda\n    assert a.dtype == torch.float16, f\"Expected torch.float16, got {a.dtype}\"\n    assert b_packed.dtype == torch.uint8\n    assert absmaxs.dtype == torch.float16\n    \n    M, K = a.shape\n    N, packed_K = b_packed.shape\n    \n    expected_packed_K = K // 2\n    assert packed_K == expected_packed_K, f\"Expected packed_K={expected_packed_K}, got {packed_K}. K must be divisible by 2\"\n    \n    c = torch.empty((M, N), device=a.device, dtype=torch.float16)\n    \n    grid = (triton.cdiv(M, 64), triton.cdiv(N, 64))\n    \n    try:\n        _matmul_fp16_int4_kernel[grid](\n            a, b_packed, absmaxs, c,\n            M, N, K,\n            a.stride(0), a.stride(1),\n            b_packed.stride(0), b_packed.stride(1),\n            c.stride(0), c.stride(1),\n        )\n    except Exception as e:\n        print(f\"Error in kernel execution: {e}\")\n        print(\"Using fallback matmul\")\n        dequantized_weights = dequantize_rowwise_int4(b_packed, absmaxs)\n        c = torch.matmul(a, dequantized_weights.t())\n    \n    return c\n\n@triton.autotune(\n    configs=[\n        triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64}, num_stages=3, num_warps=8),\n    ],\n    key=['M', 'N', 'K'],\n)\n@triton.jit\ndef _matmul_fp16_int4_kernel(\n    a_ptr, b_ptr, absmaxs_ptr, c_ptr,\n    M, N, K,\n    stride_am, stride_ak,\n    stride_bn, stride_bk,\n    stride_cm, stride_cn,\n    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n):\n    pid_m = tl.program_id(0)\n    pid_n = tl.program_id(1)\n    \n    rm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n    rn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n    \n    mask_m = rm < M\n    mask_n = rn < N\n    \n    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n    \n    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n        rk = k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n        mask_k = rk < K\n        \n        a_ptrs = a_ptr + rm[:, None] * stride_am + rk[None, :] * stride_ak\n        a_block = tl.load(a_ptrs, mask=mask_m[:, None] & mask_k[None, :], other=0.0)\n        a_block = a_block.to(tl.float32)\n        \n        packed_k = rk // 2\n        mask_packed_k = packed_k < (K // 2)\n        \n        b_ptrs = b_ptr + rn[:, None] * stride_bn + packed_k[None, :] * stride_bk\n        b_packed = tl.load(b_ptrs, mask=mask_n[:, None] & mask_packed_k[None, :], other=0)\n        \n        b_uint4_1 = (b_packed >> 4) & 0x0F\n        b_uint4_2 = b_packed & 0x0F\n        \n        b_int4_1 = (b_uint4_1.to(tl.int8) - 8).to(tl.float32)\n        b_int4_2 = (b_uint4_2.to(tl.int8) - 8).to(tl.float32)\n        \n        b_block = tl.zeros((BLOCK_SIZE_N, BLOCK_SIZE_K), dtype=tl.float32)\n        \n        even_mask = (rk % 2 == 0)[None, :]\n        odd_mask = (rk % 2 == 1)[None, :]\n        \n        b_block = tl.where(even_mask & mask_k[None, :], b_int4_1, b_block)\n        b_block = tl.where(odd_mask & mask_k[None, :], b_int4_2, b_block)\n        \n        scales_ptrs = absmaxs_ptr + rn\n        scales = tl.load(scales_ptrs, mask=mask_n, other=1.0)\n        scales = scales.to(tl.float32) / 8.0\n        \n        b_block = b_block * scales[:, None]\n        \n        b_block_t = tl.trans(b_block)\n        \n        acc += tl.dot(a_block, b_block_t, allow_tf32=False)\n    \n    c_ptrs = c_ptr + rm[:, None] * stride_cm + rn[None, :] * stride_cn\n    tl.store(c_ptrs, acc.to(tl.float16), mask=mask_m[:, None] & mask_n[None, :])\n\ndef timing_wrapper(func, input_data):\n    start_event = torch.cuda.Event(enable_timing=True)\n    end_event = torch.cuda.Event(enable_timing=True)\n    \n    start_event.record()\n    result = func(input_data)\n    end_event.record()\n    \n    torch.cuda.synchronize()\n    elapsed = start_event.elapsed_time(end_event)\n    \n    return result, elapsed\n\ndef load_testset(seed=42, seqlen=2048, tokenizer=None):\n    testdata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n    \n    if tokenizer is None:\n        tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-125m\")\n    \n    random.seed(seed)\n    np.random.seed(seed)\n    \n    test_text = \"\\n\\n\".join(testdata[\"text\"])\n    testenc = tokenizer(test_text, return_tensors='pt')\n    \n    return testenc\n\n\ndef perplexity_evaluator(model, testenc, bs=1, device=None):\n    model.seqlen = 2048\n    testenc = testenc.input_ids.to(device)\n    \n    if testenc.numel() == 0:\n        print(\"Warning: Empty testenc tensor\")\n        return float('inf')\n    \n    nsamples = testenc.numel() // model.seqlen\n    nlls = []\n    print(f\"nsamples {nsamples}\")\n    \n    for i in range(0, nsamples, bs):        \n        j = min(i + bs, nsamples)\n        inputs = testenc[:, (i * model.seqlen):(j * model.seqlen)].to(device)\n        inputs = inputs.reshape(j - i, model.seqlen)\n        \n        with torch.no_grad():\n            lm_logits = model(inputs).logits\n            \n        shift_logits = lm_logits[:, :-1, :].contiguous()\n        shift_labels = inputs[:, 1:]\n        \n        if shift_labels.numel() == 0:\n            continue\n            \n        loss_fct = torch.nn.CrossEntropyLoss()\n        loss = loss_fct(\n            shift_logits.reshape(-1, shift_logits.size(-1)), \n            shift_labels.reshape(-1)\n        )\n        \n        neg_log_likelihood = loss.float() * model.seqlen * (j - i)\n        nlls.append(neg_log_likelihood)\n    \n    if not nlls:\n        print(\"Warning: No valid batches processed\")\n        return float('inf')\n    \n    ppl = torch.exp(torch.stack(nlls).sum() / (nsamples * model.seqlen))\n    \n    torch.cuda.empty_cache()\n    \n    return ppl.item()\n\ndef evaluate_model(model, tokenizer, device=torch.device(\"cuda:0\")):\n    testenc = load_testset(tokenizer=tokenizer)\n    ppl = perplexity_evaluator(model, testenc, bs=1, device=device)\n    return ppl\n\nclass QuantizedLinearModule(torch.nn.Module):\n    def __init__(self, input_dim, output_dim, has_bias=True):\n        super(QuantizedLinearModule, self).__init__()\n        self.input_features = input_dim\n        self.output_features = output_dim\n        \n        self.weight = torch.nn.Parameter(\n            torch.empty(\n                output_dim,\n                input_dim // 2,\n                dtype=torch.uint8,\n            ),\n            requires_grad=False,\n        )\n        \n        if has_bias:\n            self.bias = torch.nn.Parameter(\n                torch.empty(\n                    self.output_features,\n                    dtype=torch.float16,\n                ),\n                requires_grad=False,\n            )\n        else:\n            self.register_parameter(\"bias\", None)\n        \n        self.register_buffer(\"weight_scale\", torch.ones(output_dim))\n\n    def forward(self, input_tensor_3d):\n        reshaped_input = input_tensor_3d.view(-1, input_tensor_3d.size(-1))\n        reshaped_input = reshaped_input.to(torch.float16)\n        \n        intermediate_result = matmul_fp16_int4(\n            reshaped_input, \n            self.weight, \n            self.weight_scale\n        ).view(*input_tensor_3d.size()[:-1], -1)\n        \n        if self.bias is not None:\n            intermediate_result = intermediate_result + self.bias\n        return intermediate_result\n    \n    @classmethod \n    def convert_from_linear(cls, original_linear):\n        assert original_linear.in_features % 2 == 0, \"Input features must be divisible by 2 for int4 packing\"\n        \n        quantized_instance = cls(\n            original_linear.in_features,\n            original_linear.out_features,\n            original_linear.bias is not None,\n        )\n    \n        if original_linear.bias is not None:\n            quantized_instance.bias = original_linear.bias.clone().to(torch.float16)\n        \n        original_weight = original_linear.weight.data.clone()\n        \n        weight_quantized, weight_scale = quantize_rowwise_int4(original_weight.to(torch.float16))\n        \n        quantized_instance.weight_scale = weight_scale.contiguous()\n        quantized_instance.weight.data = weight_quantized.contiguous()\n        \n        return quantized_instance\n\n    def __repr__(self):\n        return f'Quantized_Linear({self.input_features}, {self.output_features}, bias={self.bias is not None})'\n\ndef module_replacer(root_module):\n    module_registry = {name: module for name, module in root_module.named_modules()}\n    \n    for module_name, module_instance in module_registry.items():\n        if isinstance(module_instance, torch.nn.Linear):\n            \n            last_dot_index = module_name.rfind(\".\")\n            if last_dot_index == -1:\n                parent_module = module_registry[\"\"]\n            else:\n                parent_module = module_registry[module_name[:last_dot_index]]\n            \n            quantized_version = QuantizedLinearModule.convert_from_linear(module_instance)\n            \n            setattr(parent_module, module_name[last_dot_index + 1 :], quantized_version)\n            \n            print(f\"replace layer {module_name} with {quantized_version}\")\n            del module_instance\n\ndef benchmark():\n    loaded_model = AutoModelForCausalLM.from_pretrained(\n        \"unsloth/Llama-3.2-1B-Instruct\",\n        torch_dtype=torch.float16,\n        trust_remote_code=True,\n        device_map='cuda:0'\n    )\n    \n    tokenizer_instance = AutoTokenizer.from_pretrained(\"unsloth/Llama-3.2-1B-Instruct\")\n    \n    if not tokenizer_instance.pad_token_id:\n        tokenizer_instance.pad_token = tokenizer_instance.eos_token\n    \n    target_device = torch.device(\"cuda:0\")\n    print(f\"Using device: {target_device}\")\n    print(f\"GPU: {torch.cuda.get_device_name(target_device)}\")\n    \n    start_timer = torch.cuda.Event(enable_timing=True)\n    end_timer = torch.cuda.Event(enable_timing=True)\n    \n    start_timer.record()\n    perplexity_score = evaluate_model(loaded_model, tokenizer_instance)\n    end_timer.record()\n    torch.cuda.synchronize()\n    print(\"ppl:\", perplexity_score)\n    print(\"time (s)\"f\"{start_timer.elapsed_time(end_timer) / 1000: .2f}\")\n    \n    module_replacer(loaded_model.model)\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    start_timer.record()\n    perplexity_score_quantized = evaluate_model(loaded_model, tokenizer_instance)\n    end_timer.record()\n    torch.cuda.synchronize()\n    start_timer.elapsed_time(end_timer)\n    print(\"ppl:\", perplexity_score_quantized)\n    print(\"time (s)\"f\"{start_timer.elapsed_time(end_timer) / 1000: .2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:26:24.440190Z","iopub.execute_input":"2025-11-17T21:26:24.440494Z","iopub.status.idle":"2025-11-17T21:26:43.110155Z","shell.execute_reply.started":"2025-11-17T21:26:24.440472Z","shell.execute_reply":"2025-11-17T21:26:43.109537Z"}},"outputs":[],"execution_count":1},{"id":"7be37670","cell_type":"code","source":"benchmark()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-17T21:26:43.111199Z","iopub.execute_input":"2025-11-17T21:26:43.111654Z","iopub.status.idle":"2025-11-17T21:34:20.624863Z","shell.execute_reply.started":"2025-11-17T21:26:43.111634Z","shell.execute_reply":"2025-11-17T21:34:20.624098Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/894 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46d9da872e8f4d5eb72aba4cb04e5d86"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n2025-11-17 21:26:52.707078: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763414813.135884      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763414813.247823      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1dcb913c8ac4abcbaee846fcab994fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1f3e110b94c40a4a4115389cd7ce553"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c27924f31340465abb0f77486413db14"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6d6231c0c7340beb6f302e1b16a5f85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85c6452467ed4e5f89b111b66e3648b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6325efdd3cee4e7aa645f621609afe7d"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda:0\nGPU: Tesla T4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"331ea35377ed49279f91609c8c94a664"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c09ec6ecbd94df38613ffbad3892fa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a2cd6230024554ab11bb863c61cb8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b65f01f66d341f7b66b904f6b403e38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81c70d5e5dec42e698908fe12106eb7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1b722806f6e409e93f566765cc1ddbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0aefe4090184446f84fab2381c957e61"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (289077 > 131072). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"nsamples 141\nppl: 13.159286499023438\ntime (s) 48.89\nreplace layer layers.0.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.0.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.0.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.0.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.0.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.0.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.0.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.1.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.1.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.1.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.1.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.1.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.1.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.1.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.2.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.2.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.2.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.2.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.2.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.2.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.2.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.3.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.3.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.3.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.3.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.3.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.3.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.3.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.4.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.4.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.4.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.4.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.4.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.4.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.4.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.5.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.5.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.5.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.5.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.5.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.5.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.5.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.6.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.6.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.6.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.6.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.6.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.6.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.6.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.7.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.7.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.7.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.7.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.7.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.7.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.7.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.8.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.8.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.8.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.8.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.8.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.8.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.8.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.9.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.9.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.9.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.9.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.9.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.9.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.9.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.10.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.10.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.10.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.10.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.10.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.10.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.10.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.11.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.11.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.11.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.11.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.11.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.11.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.11.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.12.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.12.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.12.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.12.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.12.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.12.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.12.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.13.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.13.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.13.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.13.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.13.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.13.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.13.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.14.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.14.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.14.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.14.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.14.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.14.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.14.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nreplace layer layers.15.self_attn.q_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.15.self_attn.k_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.15.self_attn.v_proj with Quantized_Linear(2048, 512, bias=False)\nreplace layer layers.15.self_attn.o_proj with Quantized_Linear(2048, 2048, bias=False)\nreplace layer layers.15.mlp.gate_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.15.mlp.up_proj with Quantized_Linear(2048, 8192, bias=False)\nreplace layer layers.15.mlp.down_proj with Quantized_Linear(8192, 2048, bias=False)\nnsamples 141\nppl: 21.14326286315918\ntime (s) 352.72\n","output_type":"stream"}],"execution_count":2}]}