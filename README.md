# Реализация triton кернелей для квантизации весов в LLM и инференса квантизованной модели

## Студенты работавшие над пректом:

- Бадриев Айзат

- Кирильцев Даниил


# Этап I - ядра

## Quantization Benchmark 

Tensor Information
- **Range**: [-5.281, 5.469]
- **Shape**: 4096 × 4096

### Groupwise Quantization Benchmark

| Group Size | MAE       | Compression Ratio |
|------------|-----------|-------------------|
| 16         | 0.069818  | 3.20×             |
| 32         | 0.076317  | 3.56×             |
| 64         | 0.082923  | 3.76×             |
| 128        | 0.089438  | 3.88×             |
| 256        | 0.095812  | 3.94×             |
| 512        | 0.101973  | 3.97×             |
| 1024       | 0.107892  | 3.98×             |
| 2048       | 0.113559  | 3.99×             |
| 4096       | 0.119017  | 4.00×             |

### Symmetric Quantization Benchmark

| MAE       | Compression Ratio |
|-----------|-------------------|
| 0.102292  | 4.00×             |

### Asymmetric Quantization Benchmark
| MAE       | Compression Ratio |
|-----------|-------------------|
| 0.261056  | 3.99×             |

Воспроизвести результаты можно в файле quant_benchmark.ipynb

### Производительность матричного умножения (ускорение относительно FP16)

| n_tok | 512 × 2048 | 2048 × 2048 | 8192 × 2048 | 2048 × 8192 |
|-------|------------|-------------|-------------|-------------|
| 128   | 0.14x      | 0.13x       | 0.14x       | 0.13x       |
| 512   | 0.13x      | 0.14x       | 0.13x       | 0.13x       |
| 2048  | 0.13x      | 0.13x       | 0.13x       | 0.13x       |

Воспроизвести результаты можно в файле matmul_benchmark.ipynb

# Этап II - квантизация модели

## Сравнение с FP моделью

| Model | PPL | Time (s) |
|-------|-----|----------|
| Full-Precision | 13.15 | 50.65 |
| Quantized | 21.14 | 343.32 |

Воспроизвести результаты можно в файле edlm-wikitext-benchmark.ipynb

# Этап III - выводы

1. Удалось добиться сжатия матрицы весов в 4 раза.
2. Не удалось добиться увеличения скорости из-за неоптимальности ядра
3. Квантизация привела к увеличению перплексии, что ожидаемо.
4. Неоптимальность ядра умножения привела к снижению скорости работы модели почти в 7 раз








